{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51d69b56-31ec-497a-b67b-77fca3555287",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece2bbdb-5aa7-44a6-a0f2-710362c39dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ded2def-1198-4764-832a-fcfe2b4aa462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the data\n",
    "# file_path = \"../raw_data/all_expanded_ML.csv\"\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "# # Drop unnecessary columns\n",
    "# columns_to_drop = ['Região', 'Município', 'Estado', 'Código_UF', 'Magreza_total_%', 'UF']\n",
    "# df_dropped = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# # Filter based on 'Localização' and create separate datasets for urban and rural\n",
    "# df_urban = df_dropped[df_dropped['Localização'] == 'Urbana'].drop(columns=['Localização'])\n",
    "# df_rural = df_dropped[df_dropped['Localização'] == 'Rural'].drop(columns=['Localização'])\n",
    "\n",
    "# df_urban.to_csv(\"../raw_data/all_urban_expanded_ML.csv\")\n",
    "# df_rural.to_csv(\"../raw_data/all_rural_expanded_ML.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a44f78-2007-41c2-9866-e675e783069c",
   "metadata": {},
   "source": [
    "# DL training workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45d8cdd-5629-4912-bb86-d89b5a440932",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63356c67-4c7a-4d18-85b8-5524e43ff112",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urban = pd.read_csv(\"../raw_data/all_urban_expanded_ML.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463755d3-53c1-4398-a856-311feeec8898",
   "metadata": {},
   "source": [
    "## Create sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd32f2a-4ed1-4f92-8893-e3dbd41d2e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by 'Ano' and 'Código_IBGE' and group by 'Código_IBGE'\n",
    "df_urban_sorted = df_urban.sort_values(by=['Código_IBGE', 'Ano'])\n",
    "grouped_urban = [group for _, group in df_urban_sorted.groupby('Código_IBGE')]\n",
    "\n",
    "# Filter each group to include data from 2012 to 2020 and create sequences\n",
    "def filter_and_create_sequences(grouped_data):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for group in grouped_data:\n",
    "        filtered_group = group[(group['Ano'] >= 2012) & (group['Ano'] <= 2020)]\n",
    "        if len(filtered_group) == 9:\n",
    "            sequence = filtered_group.drop(columns=['Ano', 'Código_IBGE', 'Adjusted_funding']).values\n",
    "            target_values = filtered_group['Adjusted_funding'].values\n",
    "            sequences.append(sequence)\n",
    "            targets.append(target_values)\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "\n",
    "# Create sequences and targets for urban datasets\n",
    "array_urban, y_urban = filter_and_create_sequences(grouped_urban)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65da318b-5dd2-4514-8c9e-c7977365bbf0",
   "metadata": {},
   "source": [
    "## Create Test and Train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b9be46-e767-44cd-ab02-12b9bf3b7aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "def split_data(features, targets):\n",
    "    X_train = features[:, :6, :]\n",
    "    X_test = features[:, 6:, :]\n",
    "    y_train = targets[:, 6:]\n",
    "    y_test = targets[:, 6:]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Split the data for both urban datasets\n",
    "X_train_urban, X_test_urban, y_train_urban, y_test_urban = split_data(array_urban, y_urban)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf13b04-60ba-4e38-aa1b-40ba18ba72fe",
   "metadata": {},
   "source": [
    "\n",
    "## Train and evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1802ce3-83af-450c-8a1a-14a7fc36aa3f",
   "metadata": {},
   "source": [
    "### Working LSTM Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00c1c98-3ecb-4ef8-bcba-141839fa8c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Masking, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    " from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Scale the features\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Flatten the 3D array to 2D for scaling, then reshape back to 3D\n",
    "X_train_urban_flatten = X_train_urban.reshape(-1, X_train_urban.shape[-1])\n",
    "X_train_urban_scaled = feature_scaler.fit_transform(X_train_urban_flatten).reshape(X_train_urban.shape)\n",
    "\n",
    "X_test_urban_flatten = X_test_urban.reshape(-1, X_test_urban.shape[-1])\n",
    "X_test_urban_scaled = feature_scaler.transform(X_test_urban_flatten).reshape(X_test_urban.shape)\n",
    "\n",
    "# Scale the target variable\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "y_train_urban_scaled = target_scaler.fit_transform(y_train_urban)\n",
    "y_test_urban_scaled = target_scaler.transform(y_test_urban)\n",
    "\n",
    "X_train_urban_scaled_paded = pad_sequences(X_train_urban_scaled, dtype='float32', padding='post', value=-1000)\n",
    "# X_train_urban_scaled_paded.shape\n",
    "\n",
    "\n",
    "# Build and compile the model\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=-1000, input_shape=(None, 12)))\n",
    "\n",
    "# First LSTM layer with L2 regularization\n",
    "model.add(LSTM(units=20, activation='tanh', return_sequences=True, input_shape=(X_train_urban.shape[1], X_train_urban.shape[2]),\n",
    "               kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Second LSTM layer with L2 regularization\n",
    "model.add(LSTM(units=20, activation='tanh',\n",
    "               kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Fully connected layers with L2 regularization\n",
    "model.add(Dense(10, activation=\"relu\", kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(3, activation=\"linear\", kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "\n",
    "\n",
    "model.compile(loss='mse', optimizer='rmsprop', metrics=['mae'])\n",
    "\n",
    "# Early stopping callback\n",
    "es = EarlyStopping(patience=5)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_urban_scaled,\n",
    "    y_train_urban_scaled,\n",
    "    validation_split=0.2,\n",
    "    epochs=30,\n",
    "    batch_size=16,\n",
    "    verbose=1,\n",
    "    callbacks=[es]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(X_test_urban_scaled, y_test_urban_scaled)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# 1. Make Predictions on the scaled test features\n",
    "y_pred_scaled = model.predict(X_test_urban_scaled)\n",
    "\n",
    "# 2. Inverse Transform Predictions to original scale\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "\n",
    "# 3. Inverse Transform Actual Values to original scale\n",
    "y_test_original = target_scaler.inverse_transform(y_test_urban_scaled)\n",
    "\n",
    "# 4. Calculate Metrics on original scale\n",
    "mse_original = mean_squared_error(y_test_original, y_pred)\n",
    "mae_original = mean_absolute_error(y_test_original, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error on original scale: {mse_original}\")\n",
    "print(f\"Mean Absolute Error on original scale: {mae_original}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e9cc9a-47e6-4683-bcfa-85945743915f",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9831e138-29e1-4224-86bb-327d5253b436",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.model.save(\"../models/model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e53dd-8f5f-4a1b-b143-82924d6284a5",
   "metadata": {},
   "source": [
    "# Prediction workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8316a8b5-7f04-4b84-b0e3-42d03a7a8a66",
   "metadata": {},
   "source": [
    "## Prepare data for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98782af5-9d5e-4812-b598-83c721e01c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a specific municipality using its Código_IBGE from the original dataset\n",
    "sample_municipality_code = df_urban['Código_IBGE'].iloc[0]  # Taking the first municipality's code for demonstration\n",
    "sample_data = df_urban[df_urban['Código_IBGE'] == sample_municipality_code]\n",
    "\n",
    "# Filter the sample data to only include records from 2012 to 2020 (the range used for training)\n",
    "sample_data_filtered = sample_data[(sample_data['Ano'] >= 2012) & (sample_data['Ano'] <= 2020)]\n",
    "sample_data_filtered.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c43558-b7a3-42a7-a998-a93d936445dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Scale the features\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "sample_features_scaled = feature_scaler.fit_transform(sample_data_filtered.drop(columns=['Ano', 'Código_IBGE', 'Adjusted_funding']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5824356b-e5e4-49a0-8eaa-2c7cf2b5aefc",
   "metadata": {},
   "source": [
    "## Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d03920c-d3b5-4c36-aac7-7ed1de8c778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the pre-trained model\n",
    "new_model = load_model(\"../models/model.h5\")\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47732ae-bd53-4b9c-82f3-64bb072db947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the model\n",
    "predictions_scaled = model.predict(sample_features_scaled[-3:,:].reshape(1, 3, -1))\n",
    "\n",
    "# Inverse transform the scaled predictions to the original scale\n",
    "target_scaler = MinMaxScaler().fit()  # Assuming y_train_urban is your training target data\n",
    "predictions_original_scale = target_scaler.inverse_transform(predictions_scaled)\n",
    "predictions_original_scale\n",
    "# The predictions for the next three years are now stored in predictions_original_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dedee3-588c-4ff1-9586-4224fdb686a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbefa9f-7d89-4513-bd33-72ffe91d842d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
