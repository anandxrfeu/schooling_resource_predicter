{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fba0530-d29a-48a7-bbc8-5ccb6512a7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated function to create sequences\n",
    "def create_sequences(data):\n",
    "    grouped = data.groupby('Codigo_IBGE')\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for _, group in grouped:\n",
    "        # Sort the group by year ('Ano')\n",
    "        group = group.sort_values('Ano')\n",
    "        \n",
    "        # Drop the 'Ano' and 'Codigo_IBGE' columns as they are not features for the LSTM\n",
    "        group_data = group.drop(['Ano', 'Codigo_IBGE'], axis=1).values\n",
    "        \n",
    "        # Create sequences and labels\n",
    "        for i in range(len(group_data) - 1):\n",
    "            # Use all data up to year 'i' as the sequence to predict educational funding for year 'i+1'\n",
    "            sequence = group_data[:i+1]\n",
    "            label = group_data[i+1, -1]  # Assuming 'adjusted_funding' is the last column\n",
    "            sequences.append(sequence)\n",
    "            labels.append(label)\n",
    "            \n",
    "    return sequences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc0c38a-56df-462d-9b61-5391c653dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sequences for training and test data\n",
    "train_sequences, train_labels = create_sequences(train_data)\n",
    "test_sequences, test_labels = create_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7867d6b-e3df-48b5-89a0-9092129511f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum and maximum sequence lengths for the training set\n",
    "min_len_train = min(len(seq) for seq in train_sequences)\n",
    "max_len_train = max(len(seq) for seq in train_sequences)\n",
    "\n",
    "\n",
    "# Number of samples in the training and test sets\n",
    "num_samples_train = len(train_sequences)\n",
    "num_samples_test = len(test_sequences)\n",
    "\n",
    "# Number of time steps (This would be variable in this case, so we take the maximum sequence length)\n",
    "num_time_steps_train = max_len_train\n",
    "num_time_steps_test = max(len(seq) for seq in test_sequences)\n",
    "\n",
    "# Number of features (excluding 'Ano' and 'Codigo_IBGE')\n",
    "num_features = train_data.shape[1] - 2\n",
    "\n",
    "num_samples_train, num_time_steps_train, num_features, num_samples_test, num_time_steps_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed1e8c4-da83-46b8-975c-77dd40d52383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pad sequences using a constant value of 1000\n",
    "train_sequences_padded = pad_sequences(train_sequences, padding='post', value=1000, dtype='float32')\n",
    "test_sequences_padded = pad_sequences(test_sequences, padding='post', value=1000, dtype='float32')\n",
    "\n",
    "# The resulting `train_sequences_padded` and `test_sequences_padded` will be numpy arrays\n",
    "# that you can directly use for training your LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5683dffa-47eb-4505-870e-85435d803f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, layers\n",
    "\n",
    "# Model\n",
    "model = Sequential()\n",
    "\n",
    "# Masking layer to ignore padding\n",
    "model.add(layers.Masking(mask_value=1000, input_shape=(num_time_steps_train, num_features)))\n",
    "\n",
    "# Simple RNN layer\n",
    "model.add(layers.SimpleRNN(units=2, activation='tanh'))\n",
    "\n",
    "# Additional dense layers\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compilation\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "# Fit the model\n",
    "# Convert train_labels to a NumPy array with dtype 'float32'\n",
    "train_labels_array = np.array(train_labels, dtype='float32')\n",
    "\n",
    "# Fit the model\n",
    "model.fit(train_sequences_padded, train_labels_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e26ce9-bc10-4f38-8326-c0934f1826ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803422eb-597d-4e93-bbcb-5cc333e39a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "\n",
    "# Sample future data for prediction\n",
    "future_data = {\n",
    "    'Ano': [2021],\n",
    "    'Codigo_IBGE': [1100015],\n",
    "    'Aprovacao': [98.4],\n",
    "    'Reprovacao': [1.6],\n",
    "    'Abandono': [0],\n",
    "    'Matriculas': [749],\n",
    "    'Docentes': [71],\n",
    "    'Estabelecimentos': [3],\n",
    "    'Turmas': [45],\n",
    "    'PIB': [28722.45],\n",
    "    'Poverty_%': [8895900.54],\n",
    "    'Unemployed_%': [19318.8],\n",
    "    'Acesso a internet %': [0],\n",
    "    'adjusted_population': [0]\n",
    "}\n",
    "\n",
    "# Convert the sample data to a DataFrame\n",
    "future_df = pd.DataFrame(future_data)\n",
    "\n",
    "# Scale the features using the same scaler object used for training data\n",
    "future_df_scaled = future_df.copy()\n",
    "future_df_scaled[columns_to_scale] = scaler.transform(future_df[columns_to_scale])\n",
    "\n",
    "# Drop the 'Ano' and 'Codigo_IBGE' columns as they are not features for the LSTM\n",
    "future_sequence = future_df_scaled.drop(['Ano', 'Codigo_IBGE'], axis=1).values\n",
    "\n",
    "# Since the LSTM expects input shape [samples, time_steps, features], reshape the sequence accordingly\n",
    "future_sequence_reshaped = np.expand_dims(future_sequence, axis=0)\n",
    "\n",
    "# Use the LSTM model to make the prediction\n",
    "predicted_value = model.predict(future_sequence_reshaped)\n",
    "\n",
    "# The 'predicted_value' will contain the predicted 'adjusted_funding' for 2021 for the municipality with code 1100015\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d158ae26-2218-43c8-af42-9f26293a6900",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "99569c26-8981-41a3-bbdf-56fada5e72a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class LSTMPipeline:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.feature_scaler = None\n",
    "        self.target_scaler = None\n",
    "        self.model = None\n",
    "        self.sequences_padded = None\n",
    "        self.scaled_df = None\n",
    "\n",
    "    def preprocess_data(self, df, target_column):\n",
    "        df_cleaned = df.dropna().drop_duplicates()\n",
    "        columns_to_scale = df_cleaned.columns.difference(['Ano', 'Codigo_IBGE', target_column])\n",
    "\n",
    "        if self.feature_scaler is None:\n",
    "            self.feature_scaler = MinMaxScaler()\n",
    "        df_scaled = df_cleaned.copy()\n",
    "        df_scaled[columns_to_scale] = self.feature_scaler.fit_transform(df_cleaned[columns_to_scale])\n",
    "        \n",
    "        if target_column in df_cleaned.columns:\n",
    "            if self.target_scaler is None:\n",
    "                self.target_scaler = MinMaxScaler()\n",
    "            df_scaled[[target_column]] = self.target_scaler.fit_transform(df_cleaned[[target_column]])\n",
    "\n",
    "        return df_scaled\n",
    "\n",
    "    def create_sequences(self, data, target_column):\n",
    "        grouped = data.groupby('Codigo_IBGE')\n",
    "        sequences = []\n",
    "        labels = []\n",
    "        for _, group in grouped:\n",
    "            group = group.sort_values('Ano')\n",
    "            group_data = group.drop(['Ano', 'Codigo_IBGE', target_column], axis=1).values  # Drop the target column here\n",
    "            #print(f\"Shape of group_data for municipality {group['Codigo_IBGE'].iloc[0]}: {group_data.shape}\")  # Debugging line\n",
    "            group_labels = group[target_column].values  # Extract the labels (target column)\n",
    "            for i in range(len(group_data) - 1):\n",
    "                sequence = group_data[:i+1]\n",
    "                label = group_labels[i+1]  # Use the label corresponding to the next timestep\n",
    "                sequences.append(sequence)\n",
    "                labels.append(label)\n",
    "        return sequences, labels\n",
    "\n",
    "\n",
    "    def pad_sequences(self, sequences, pad_value=1000):\n",
    "        return pad_sequences(sequences, dtype='float32', padding='post', value=pad_value)\n",
    "\n",
    "    def build_and_train_model(self, X, y):\n",
    "        input_shape = (None, X.shape[-1])\n",
    "        print(input_shape)\n",
    "        model = Sequential()\n",
    "        model.add(layers.Masking(mask_value=1000, input_shape=input_shape))\n",
    "        model.add(layers.SimpleRNN(units=2, activation='tanh'))\n",
    "        model.add(layers.Dense(10, activation='relu'))\n",
    "        model.add(layers.Dense(1, activation='linear'))\n",
    "        model.compile(loss='mae', optimizer='adam')\n",
    "        y_array = np.array(y, dtype='float32')\n",
    "        model.fit(X, y_array)\n",
    "        self.sequences_padded = X\n",
    "        self.model = model\n",
    "\n",
    "    \n",
    "    def get_last_sequence_for_municipality(self, municipality_code, sequences, data):\n",
    "        data_filtered = data[data['Codigo_IBGE'] == municipality_code].sort_values('Ano')\n",
    "        last_year = data_filtered['Ano'].max()\n",
    "        municipality_sequences = [seq for seq, (_, group) in zip(sequences, data.groupby('Codigo_IBGE')) if group['Codigo_IBGE'].iloc[0] == municipality_code]\n",
    "        if not municipality_sequences:\n",
    "            return None  \n",
    "        last_sequence = municipality_sequences[-1]\n",
    "        return last_sequence\n",
    "\n",
    "\n",
    "\n",
    "    def inverse_transform_prediction(self, prediction):\n",
    "        if self.target_scaler is None:\n",
    "            raise AttributeError(\"target_scaler has not been initialized. Make sure to preprocess the data with the target column first.\")\n",
    "        prediction = np.array(prediction).reshape(-1, 1)\n",
    "        prediction_inverse_transformed = self.target_scaler.inverse_transform(prediction)\n",
    "        return prediction_inverse_transformed[0][0]\n",
    "\n",
    "    def predict(self, future_data):\n",
    "        # Preprocess the future data\n",
    "        scaled_future_data = self.preprocess_data(future_data, 'adjusted_funding')\n",
    "        \n",
    "        # Fetch the last sequence for each municipality in the future data\n",
    "        predictions = {}\n",
    "        for code in scaled_future_data['Codigo_IBGE'].unique():\n",
    "\n",
    "            last_sequence = self.get_last_sequence_for_municipality(code, self.sequences_padded, self.scaled_df)\n",
    "            \n",
    "            # Extend the last sequence with the new data point\n",
    "            future_point = scaled_future_data[scaled_future_data['Codigo_IBGE'] == code].drop(['Ano', 'Codigo_IBGE'], axis=1).values\n",
    "    \n",
    "            #print(\"Shape of future_point:\", future_point.shape)\n",
    "            \n",
    "            extended_sequence = np.vstack([last_sequence, future_point])\n",
    "            \n",
    "            # Pad the sequence\n",
    "            extended_sequence_padded = self.pad_sequences([extended_sequence])\n",
    "            #print(f\"Extended sequence shape: {extended_sequence.shape}\")  # Debugging line\n",
    "            #print(f\"Padded sequence shape: {extended_sequence_padded.shape}\")  # Debugging line\n",
    "            \n",
    "            # Make the prediction\n",
    "            prediction = self.model.predict(extended_sequence_padded)\n",
    "            prediction_inverse_transformed = self.inverse_transform_prediction(prediction)\n",
    "            predictions[code] = prediction_inverse_transformed\n",
    "            \n",
    "        return predictions\n",
    "\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        if self.model:\n",
    "            self.model.save(filepath)\n",
    "        else:\n",
    "            print(\"Model is not trained yet.\")\n",
    "\n",
    "    def load_saved_model(self, filepath):\n",
    "        try:\n",
    "            return tf.keras.models.load_model(filepath)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while loading the model: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d8140-ca65-4c27-9c04-d4be369046a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233cb407-404f-406f-9d1d-d57d2e51db73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79b3b15-240b-4dd7-ac9b-9b1ecc470e24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361f9822-531b-49e4-b4a6-d47280116230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7f551454-72eb-4914-8681-8304fcccc67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 12)\n",
      "1314/1314 [==============================] - 2s 1ms/step - loss: 0.0012\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../raw_data/all_urban_ML2.csv\")\n",
    "\n",
    "# Example usage:\n",
    "pipeline = LSTMPipeline()\n",
    "\n",
    "# # Preprocessing and \n",
    "pipeline.scaled_df = pipeline.preprocess_data(df, 'adjusted_funding')\n",
    "\n",
    "# #sequence creation\n",
    "sequences, labels = pipeline.create_sequences(pipeline.scaled_df, 'adjusted_funding')\n",
    "\n",
    "# # Sequence padding \n",
    "sequences_padded = pipeline.pad_sequences(sequences)\n",
    "\n",
    "# # Model building and training\n",
    "pipeline.build_and_train_model(sequences_padded, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea1ce1dd-8757-409d-be8c-f4787b4c3d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_data = {\n",
    "    'Ano': [2022],\n",
    "    'Codigo_IBGE': [1100015],\n",
    "    'Aprovacao': [98.4],\n",
    "    'Reprovacao': [1.6],\n",
    "    'Abandono': [0.0],\n",
    "    'Matriculas': [749],\n",
    "    'Docentes': [71],\n",
    "    'Estabelecimentos': [3],\n",
    "    'Turmas': [45],\n",
    "    'PIB': [28722.45],\n",
    "    'Poverty_%': [19.7],\n",
    "    'Unemployed_%': [10.43],\n",
    "    'Acesso a internet %': [81],\n",
    "    'adjusted_population': [19318.8]\n",
    "}\n",
    "\n",
    "# Convert the sample data to a DataFrame\n",
    "future_df = pd.DataFrame(future_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "04722bdd-5db8-4a2c-aefe-1c522ff41144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 69ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1100015: 27503316.0}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = pipeline.predict(future_df)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea28b05-74e7-46a0-8897-a9fe875e5b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cadba8-2554-49ac-8356-427bc0add7b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "923f243f-1c3f-49fc-8b83-fb32c045a4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anandxrfeu/.pyenv/versions/3.10.6/envs/schooling_resource_predicter/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "filepath = \"../models/model.h5\"\n",
    "pipeline.save_model(\"../models/model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c7e499c4-895f-4dfa-b93f-ca6a49752ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking_7 (Masking)         (None, None, 12)          0         \n",
      "                                                                 \n",
      " simple_rnn_7 (SimpleRNN)    (None, 2)                 30        \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 10)                30        \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71 (284.00 Byte)\n",
      "Trainable params: 71 (284.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mymodel = pipeline.load_saved_model(filepath)\n",
    "mymodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1810985-b3ed-44eb-ac70-3580d826cf25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a7442adf-7c0e-40b8-a774-5660bbbc0eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../raw_data/all_urban_ML2.csv\")\n",
    "\n",
    "# Example usage:\n",
    "pipeline = LSTMPipeline()\n",
    "\n",
    "# # Preprocessing and \n",
    "pipeline.scaled_df = pipeline.preprocess_data(df, 'adjusted_funding')\n",
    "\n",
    "# #sequence creation\n",
    "sequences, labels = pipeline.create_sequences(pipeline.scaled_df, 'adjusted_funding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1a83a032-7969-423a-88ff-329a80fa1bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sequence padding \n",
    "sequences_padded = pipeline.pad_sequences(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "723cc863-e0c4-4f1f-9f5c-f2184950a2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 12)\n",
      "1314/1314 [==============================] - 2s 1ms/step - loss: 0.0137\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Model building and training\n",
    "pipeline.build_and_train_model(sequences_padded, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "da906e7a-6af3-4023-a373-595b0aa7bfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_data = {\n",
    "    'Ano': [2022],\n",
    "    'Codigo_IBGE': [1100015],\n",
    "    'Aprovacao': [98.4],\n",
    "    'Reprovacao': [1.6],\n",
    "    'Abandono': [0.0],\n",
    "    'Matriculas': [749],\n",
    "    'Docentes': [71],\n",
    "    'Estabelecimentos': [3],\n",
    "    'Turmas': [45],\n",
    "    'PIB': [28722.45],\n",
    "    'Poverty_%': [19.7],\n",
    "    'Unemployed_%': [10.43],\n",
    "    'Acesso a internet %': [81],\n",
    "    'adjusted_population': [19318.8]\n",
    "}\n",
    "\n",
    "# Convert the sample data to a DataFrame\n",
    "future_df = pd.DataFrame(future_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8846e54e-ac6c-4dbe-bde9-b53a73a98b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 71ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1100015: 43660770.0}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict(future_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "96b12592-082c-4a22-8995-975aa38b2f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anandxrfeu/.pyenv/versions/3.10.6/envs/schooling_resource_predicter/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "filepath = \"../models/model.h5\"\n",
    "pipeline.save_model(\"../models/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0f114034-cb6f-4308-a9d2-c2ccb5b2a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = pipeline.load_saved_model(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d6bbd941-e98b-47a9-a06a-50100adb2255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking_9 (Masking)         (None, None, 12)          0         \n",
      "                                                                 \n",
      " simple_rnn_9 (SimpleRNN)    (None, 2)                 30        \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 10)                30        \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71 (284.00 Byte)\n",
      "Trainable params: 71 (284.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mymodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a918b8bd-3d51-4a61-be21-2df91cb77f85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
