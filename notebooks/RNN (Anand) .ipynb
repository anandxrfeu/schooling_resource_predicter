{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fba0530-d29a-48a7-bbc8-5ccb6512a7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated function to create sequences\n",
    "def create_sequences(data):\n",
    "    grouped = data.groupby('Codigo_IBGE')\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for _, group in grouped:\n",
    "        # Sort the group by year ('Ano')\n",
    "        group = group.sort_values('Ano')\n",
    "        \n",
    "        # Drop the 'Ano' and 'Codigo_IBGE' columns as they are not features for the LSTM\n",
    "        group_data = group.drop(['Ano', 'Codigo_IBGE'], axis=1).values\n",
    "        \n",
    "        # Create sequences and labels\n",
    "        for i in range(len(group_data) - 1):\n",
    "            # Use all data up to year 'i' as the sequence to predict educational funding for year 'i+1'\n",
    "            sequence = group_data[:i+1]\n",
    "            label = group_data[i+1, -1]  # Assuming 'adjusted_funding' is the last column\n",
    "            sequences.append(sequence)\n",
    "            labels.append(label)\n",
    "            \n",
    "    return sequences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc0c38a-56df-462d-9b61-5391c653dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sequences for training and test data\n",
    "train_sequences, train_labels = create_sequences(train_data)\n",
    "test_sequences, test_labels = create_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7867d6b-e3df-48b5-89a0-9092129511f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum and maximum sequence lengths for the training set\n",
    "min_len_train = min(len(seq) for seq in train_sequences)\n",
    "max_len_train = max(len(seq) for seq in train_sequences)\n",
    "\n",
    "\n",
    "# Number of samples in the training and test sets\n",
    "num_samples_train = len(train_sequences)\n",
    "num_samples_test = len(test_sequences)\n",
    "\n",
    "# Number of time steps (This would be variable in this case, so we take the maximum sequence length)\n",
    "num_time_steps_train = max_len_train\n",
    "num_time_steps_test = max(len(seq) for seq in test_sequences)\n",
    "\n",
    "# Number of features (excluding 'Ano' and 'Codigo_IBGE')\n",
    "num_features = train_data.shape[1] - 2\n",
    "\n",
    "num_samples_train, num_time_steps_train, num_features, num_samples_test, num_time_steps_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed1e8c4-da83-46b8-975c-77dd40d52383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pad sequences using a constant value of 1000\n",
    "train_sequences_padded = pad_sequences(train_sequences, padding='post', value=1000, dtype='float32')\n",
    "test_sequences_padded = pad_sequences(test_sequences, padding='post', value=1000, dtype='float32')\n",
    "\n",
    "# The resulting `train_sequences_padded` and `test_sequences_padded` will be numpy arrays\n",
    "# that you can directly use for training your LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5683dffa-47eb-4505-870e-85435d803f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, layers\n",
    "\n",
    "# Model\n",
    "model = Sequential()\n",
    "\n",
    "# Masking layer to ignore padding\n",
    "model.add(layers.Masking(mask_value=1000, input_shape=(num_time_steps_train, num_features)))\n",
    "\n",
    "# Simple RNN layer\n",
    "model.add(layers.SimpleRNN(units=2, activation='tanh'))\n",
    "\n",
    "# Additional dense layers\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compilation\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "# Fit the model\n",
    "# Convert train_labels to a NumPy array with dtype 'float32'\n",
    "train_labels_array = np.array(train_labels, dtype='float32')\n",
    "\n",
    "# Fit the model\n",
    "model.fit(train_sequences_padded, train_labels_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e26ce9-bc10-4f38-8326-c0934f1826ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803422eb-597d-4e93-bbcb-5cc333e39a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "\n",
    "# Sample future data for prediction\n",
    "future_data = {\n",
    "    'Ano': [2021],\n",
    "    'Codigo_IBGE': [1100015],\n",
    "    'Aprovacao': [98.4],\n",
    "    'Reprovacao': [1.6],\n",
    "    'Abandono': [0],\n",
    "    'Matriculas': [749],\n",
    "    'Docentes': [71],\n",
    "    'Estabelecimentos': [3],\n",
    "    'Turmas': [45],\n",
    "    'PIB': [28722.45],\n",
    "    'Poverty_%': [8895900.54],\n",
    "    'Unemployed_%': [19318.8],\n",
    "    'Acesso a internet %': [0],\n",
    "    'adjusted_population': [0]\n",
    "}\n",
    "\n",
    "# Convert the sample data to a DataFrame\n",
    "future_df = pd.DataFrame(future_data)\n",
    "\n",
    "# Scale the features using the same scaler object used for training data\n",
    "future_df_scaled = future_df.copy()\n",
    "future_df_scaled[columns_to_scale] = scaler.transform(future_df[columns_to_scale])\n",
    "\n",
    "# Drop the 'Ano' and 'Codigo_IBGE' columns as they are not features for the LSTM\n",
    "future_sequence = future_df_scaled.drop(['Ano', 'Codigo_IBGE'], axis=1).values\n",
    "\n",
    "# Since the LSTM expects input shape [samples, time_steps, features], reshape the sequence accordingly\n",
    "future_sequence_reshaped = np.expand_dims(future_sequence, axis=0)\n",
    "\n",
    "# Use the LSTM model to make the prediction\n",
    "predicted_value = model.predict(future_sequence_reshaped)\n",
    "\n",
    "# The 'predicted_value' will contain the predicted 'adjusted_funding' for 2021 for the municipality with code 1100015\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d158ae26-2218-43c8-af42-9f26293a6900",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590c6825-5444-4305-939f-9ec93586ad56",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Class implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99569c26-8981-41a3-bbdf-56fada5e72a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class LSTMPipeline:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.feature_scaler = None\n",
    "        self.target_scaler = None\n",
    "        self.model = None\n",
    "        self.sequences_padded = None\n",
    "        self.scaled_df = None\n",
    "\n",
    "    def preprocess_data(self, df, target_column):\n",
    "        df_cleaned = df.dropna().drop_duplicates()\n",
    "        columns_to_scale = df_cleaned.columns.difference(['Ano', 'Codigo_IBGE', target_column])\n",
    "\n",
    "        if self.feature_scaler is None:\n",
    "            self.feature_scaler = MinMaxScaler()\n",
    "        df_scaled = df_cleaned.copy()\n",
    "        df_scaled[columns_to_scale] = self.feature_scaler.fit_transform(df_cleaned[columns_to_scale])\n",
    "        \n",
    "        if target_column in df_cleaned.columns:\n",
    "            if self.target_scaler is None:\n",
    "                self.target_scaler = MinMaxScaler()\n",
    "            df_scaled[[target_column]] = self.target_scaler.fit_transform(df_cleaned[[target_column]])\n",
    "\n",
    "        return df_scaled\n",
    "\n",
    "    def create_sequences(self, data, target_column):\n",
    "        grouped = data.groupby('Codigo_IBGE')\n",
    "        sequences = []\n",
    "        labels = []\n",
    "        for _, group in grouped:\n",
    "            group = group.sort_values('Ano')\n",
    "            group_data = group.drop(['Ano', 'Codigo_IBGE', target_column], axis=1).values  # Drop the target column here\n",
    "            #print(f\"Shape of group_data for municipality {group['Codigo_IBGE'].iloc[0]}: {group_data.shape}\")  # Debugging line\n",
    "            group_labels = group[target_column].values  # Extract the labels (target column)\n",
    "            for i in range(len(group_data) - 1):\n",
    "                sequence = group_data[:i+1]\n",
    "                label = group_labels[i+1]  # Use the label corresponding to the next timestep\n",
    "                sequences.append(sequence)\n",
    "                labels.append(label)\n",
    "        return sequences, labels\n",
    "\n",
    "\n",
    "    def pad_sequences(self, sequences, pad_value=1000):\n",
    "        return pad_sequences(sequences, dtype='float32', padding='post', value=pad_value)\n",
    "\n",
    "    def build_and_train_model(self, X, y):\n",
    "        input_shape = (None, X.shape[-1])\n",
    "        print(input_shape)\n",
    "        model = Sequential()\n",
    "        model.add(layers.Masking(mask_value=1000, input_shape=input_shape))\n",
    "        model.add(layers.SimpleRNN(units=2, activation='tanh'))\n",
    "        model.add(layers.Dense(10, activation='relu'))\n",
    "        model.add(layers.Dense(1, activation='linear'))\n",
    "        model.compile(loss='mae', optimizer='adam')\n",
    "        y_array = np.array(y, dtype='float32')\n",
    "        model.fit(X, y_array)\n",
    "        self.sequences_padded = X\n",
    "        self.model = model\n",
    "\n",
    "    \n",
    "    def get_last_sequence_for_municipality(self, municipality_code, sequences, data):\n",
    "        data_filtered = data[data['Codigo_IBGE'] == municipality_code].sort_values('Ano')\n",
    "        last_year = data_filtered['Ano'].max()\n",
    "        municipality_sequences = [seq for seq, (_, group) in zip(sequences, data.groupby('Codigo_IBGE')) if group['Codigo_IBGE'].iloc[0] == municipality_code]\n",
    "        if not municipality_sequences:\n",
    "            return None  \n",
    "        last_sequence = municipality_sequences[-1]\n",
    "        return last_sequence\n",
    "\n",
    "\n",
    "\n",
    "    def inverse_transform_prediction(self, prediction):\n",
    "        if self.target_scaler is None:\n",
    "            raise AttributeError(\"target_scaler has not been initialized. Make sure to preprocess the data with the target column first.\")\n",
    "        prediction = np.array(prediction).reshape(-1, 1)\n",
    "        prediction_inverse_transformed = self.target_scaler.inverse_transform(prediction)\n",
    "        return prediction_inverse_transformed[0][0]\n",
    "\n",
    "    def predict(self, future_data):\n",
    "        # Preprocess the future data\n",
    "        scaled_future_data = self.preprocess_data(future_data, 'adjusted_funding')\n",
    "        \n",
    "        # Fetch the last sequence for each municipality in the future data\n",
    "        predictions = {}\n",
    "        for code in scaled_future_data['Codigo_IBGE'].unique():\n",
    "\n",
    "            last_sequence = self.get_last_sequence_for_municipality(code, self.sequences_padded, self.scaled_df)\n",
    "            \n",
    "            # Extend the last sequence with the new data point\n",
    "            future_point = scaled_future_data[scaled_future_data['Codigo_IBGE'] == code].drop(['Ano', 'Codigo_IBGE'], axis=1).values\n",
    "    \n",
    "            #print(\"Shape of future_point:\", future_point.shape)\n",
    "            \n",
    "            extended_sequence = np.vstack([last_sequence, future_point])\n",
    "            \n",
    "            # Pad the sequence\n",
    "            extended_sequence_padded = self.pad_sequences([extended_sequence])\n",
    "            #print(f\"Extended sequence shape: {extended_sequence.shape}\")  # Debugging line\n",
    "            #print(f\"Padded sequence shape: {extended_sequence_padded.shape}\")  # Debugging line\n",
    "            \n",
    "            # Make the prediction\n",
    "            prediction = self.model.predict(extended_sequence_padded)\n",
    "            prediction_inverse_transformed = self.inverse_transform_prediction(prediction)\n",
    "            predictions[code] = prediction_inverse_transformed\n",
    "            \n",
    "        return predictions\n",
    "\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        if self.model:\n",
    "            self.model.save(filepath)\n",
    "        else:\n",
    "            print(\"Model is not trained yet.\")\n",
    "\n",
    "    def load_saved_model(self, filepath):\n",
    "        try:\n",
    "            return tf.keras.models.load_model(filepath)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while loading the model: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f551454-72eb-4914-8681-8304fcccc67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../raw_data/all_urban_ML2.csv\")\n",
    "\n",
    "# Example usage:\n",
    "pipeline = LSTMPipeline()\n",
    "\n",
    "# # Preprocessing and \n",
    "pipeline.scaled_df = pipeline.preprocess_data(df, 'adjusted_funding')\n",
    "\n",
    "# #sequence creation\n",
    "sequences, labels = pipeline.create_sequences(pipeline.scaled_df, 'adjusted_funding')\n",
    "\n",
    "# # Sequence padding \n",
    "sequences_padded = pipeline.pad_sequences(sequences)\n",
    "\n",
    "# # Model building and training\n",
    "pipeline.build_and_train_model(sequences_padded, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1ce1dd-8757-409d-be8c-f4787b4c3d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_data = {\n",
    "    'Ano': [2022],\n",
    "    'Codigo_IBGE': [1100015],\n",
    "    'Aprovacao': [98.4],\n",
    "    'Reprovacao': [1.6],\n",
    "    'Abandono': [0.0],\n",
    "    'Matriculas': [749],\n",
    "    'Docentes': [71],\n",
    "    'Estabelecimentos': [3],\n",
    "    'Turmas': [45],\n",
    "    'PIB': [28722.45],\n",
    "    'Poverty_%': [19.7],\n",
    "    'Unemployed_%': [10.43],\n",
    "    'Acesso a internet %': [81],\n",
    "    'adjusted_population': [19318.8]\n",
    "}\n",
    "\n",
    "# Convert the sample data to a DataFrame\n",
    "future_df = pd.DataFrame(future_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04722bdd-5db8-4a2c-aefe-1c522ff41144",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pipeline.predict(future_df)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923f243f-1c3f-49fc-8b83-fb32c045a4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../models/model.h5\"\n",
    "pipeline.save_model(\"../models/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f114034-cb6f-4308-a9d2-c2ccb5b2a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = pipeline.load_saved_model(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bbd941-e98b-47a9-a06a-50100adb2255",
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a918b8bd-3d51-4a61-be21-2df91cb77f85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8018244d-2066-4ba5-97e4-079be441ef13",
   "metadata": {},
   "source": [
    "### Function implementaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "412c393b-8ad5-492c-83d5-da2f12549bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Global Variables\n",
    "feature_scaler = None\n",
    "target_scaler = None\n",
    "model = None\n",
    "sequences_padded = None\n",
    "scaled_df = None\n",
    "\n",
    "def preprocess_data(df, target_column):\n",
    "    global feature_scaler\n",
    "    global target_scaler\n",
    "\n",
    "    df_cleaned = df.dropna().drop_duplicates()\n",
    "    columns_to_scale = df_cleaned.columns.difference(['Ano', 'Codigo_IBGE', target_column])\n",
    "\n",
    "    if feature_scaler is None:\n",
    "        feature_scaler = MinMaxScaler()\n",
    "    \n",
    "    df_scaled = df_cleaned.copy()\n",
    "    df_scaled[columns_to_scale] = feature_scaler.fit_transform(df_cleaned[columns_to_scale])\n",
    "    \n",
    "    if target_column in df_cleaned.columns:\n",
    "        if target_scaler is None:\n",
    "            target_scaler = MinMaxScaler()\n",
    "        df_scaled[[target_column]] = target_scaler.fit_transform(df_cleaned[[target_column]])\n",
    "\n",
    "    return df_scaled\n",
    "\n",
    "\n",
    "def create_sequences(data, target_column):\n",
    "    grouped = data.groupby('Codigo_IBGE')\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    for _, group in grouped:\n",
    "        group = group.sort_values('Ano')\n",
    "        group_data = group.drop(['Ano', 'Codigo_IBGE', target_column], axis=1).values\n",
    "        group_labels = group[target_column].values\n",
    "        for i in range(len(group_data) - 1):\n",
    "            sequence = group_data[:i+1]\n",
    "            label = group_labels[i+1]\n",
    "            sequences.append(sequence)\n",
    "            labels.append(label)\n",
    "\n",
    "    return sequences, labels\n",
    "\n",
    "\n",
    "def add_padding(sequences, pad_value=1000):\n",
    "    return pad_sequences(sequences, dtype='float32', padding='post', value=pad_value)\n",
    "\n",
    "\n",
    "def build_and_train_model(X, y):\n",
    "    global model\n",
    "    global sequences_padded\n",
    "\n",
    "    input_shape = (None, X.shape[-1])\n",
    "    model = Sequential()\n",
    "    model.add(layers.Masking(mask_value=1000, input_shape=input_shape))\n",
    "    model.add(layers.SimpleRNN(units=2, activation='tanh'))\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    y_array = np.array(y, dtype='float32')\n",
    "    model.fit(X, y_array)\n",
    "    sequences_padded = X\n",
    "\n",
    "\n",
    "def get_last_sequence_for_municipality(municipality_code, sequences, data):\n",
    "    data_filtered = data[data['Codigo_IBGE'] == municipality_code].sort_values('Ano')\n",
    "    last_year = data_filtered['Ano'].max()\n",
    "    municipality_sequences = [seq for seq, (_, group) in zip(sequences, data.groupby('Codigo_IBGE')) if group['Codigo_IBGE'].iloc[0] == municipality_code]\n",
    "    if not municipality_sequences:\n",
    "        return None\n",
    "    last_sequence = municipality_sequences[-1]\n",
    "    return last_sequence\n",
    "\n",
    "def inverse_transform_prediction(prediction):\n",
    "    global target_scaler\n",
    "    if target_scaler is None:\n",
    "        raise AttributeError(\"target_scaler has not been initialized. Make sure to preprocess the data with the target column first.\")\n",
    "    prediction = np.array(prediction).reshape(-1, 1)\n",
    "    prediction_inverse_transformed = target_scaler.inverse_transform(prediction)\n",
    "    return prediction_inverse_transformed[0][0]\n",
    "\n",
    "\n",
    "def predict(future_data):\n",
    "    global sequences_padded\n",
    "    global scaled_df\n",
    "    global model\n",
    "\n",
    "    # Preprocess the future data\n",
    "    scaled_future_data = preprocess_data(future_data, 'adjusted_funding')\n",
    "    \n",
    "    # Fetch the last sequence for each municipality in the future data\n",
    "    predictions = {}\n",
    "    for code in scaled_future_data['Codigo_IBGE'].unique():\n",
    "        last_sequence = get_last_sequence_for_municipality(code, sequences_padded, scaled_df)\n",
    "        \n",
    "        # Extend the last sequence with the new data point\n",
    "        future_point = scaled_future_data[scaled_future_data['Codigo_IBGE'] == code].drop(['Ano', 'Codigo_IBGE'], axis=1).values\n",
    "\n",
    "        extended_sequence = np.vstack([last_sequence, future_point])\n",
    "        \n",
    "        # Pad the sequence\n",
    "        extended_sequence_padded = pad_sequences([extended_sequence])\n",
    "        \n",
    "        # Make the prediction\n",
    "        prediction = model.predict(extended_sequence_padded)\n",
    "        prediction_inverse_transformed = inverse_transform_prediction(prediction)\n",
    "        predictions[code] = prediction_inverse_transformed\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def save_model(filepath):\n",
    "    global model\n",
    "    if model:\n",
    "        model.save(filepath)\n",
    "    else:\n",
    "        print(\"Model is not trained yet.\")\n",
    "\n",
    "\n",
    "def load_saved_model(filepath):\n",
    "    try:\n",
    "        loaded_model = tf.keras.models.load_model(filepath)\n",
    "        return loaded_model\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the model: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "259bc589-bd09-4f2d-823b-0b4ec240d729",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../raw_data/all_urban_ML2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e80fb0a5-d65e-46cb-9e1e-6d8af8fe1c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1314/1314 [==============================] - 2s 1ms/step - loss: 0.0093\n"
     ]
    }
   ],
   "source": [
    "# # Preprocessing and \n",
    "scaled_df = preprocess_data(df, 'adjusted_funding')\n",
    "# # Create sequences\n",
    "sequences, labels = create_sequences(scaled_df, 'adjusted_funding')\n",
    "# # Sequence padding \n",
    "sequences_padded = add_padding(sequences)\n",
    "# # Model building and training\n",
    "build_and_train_model(sequences_padded, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2aca399-bf08-4855-95e3-fc8d1b3baede",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_data = {\n",
    "    'Ano': [2022],\n",
    "    'Codigo_IBGE': [1100015],\n",
    "    'Aprovacao': [98.4],\n",
    "    'Reprovacao': [1.6],\n",
    "    'Abandono': [0.0],\n",
    "    'Matriculas': [749],\n",
    "    'Docentes': [71],\n",
    "    'Estabelecimentos': [3],\n",
    "    'Turmas': [45],\n",
    "    'PIB': [28722.45],\n",
    "    'Poverty_%': [19.7],\n",
    "    'Unemployed_%': [10.43],\n",
    "    'Acesso a internet %': [81],\n",
    "    'adjusted_population': [19318.8]\n",
    "}\n",
    "\n",
    "# Convert the sample data to a DataFrame\n",
    "future_df = pd.DataFrame(future_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2506d7a-0695-445c-b7a3-3ed7e2b4b007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 88ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1100015: -165759360.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = predict(future_df)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19fc7aa7-9a60-4bb3-94ed-93c1ca55c632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anandxrfeu/.pyenv/versions/3.10.6/envs/schooling_resource_predicter/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "filepath = \"../models/model.h5\"\n",
    "save_model(\"../models/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c235326c-1c57-4569-98e9-18c2fe9d0d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking_1 (Masking)         (None, None, 12)          0         \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 2)                 30        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                30        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71 (284.00 Byte)\n",
      "Trainable params: 71 (284.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mymodel = load_saved_model(filepath)\n",
    "mymodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a96a8c2-5312-44dc-8ea6-3e44da9118b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 72ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1100015: -165759360.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess the future data\n",
    "scaled_future_data = preprocess_data(future_df, 'adjusted_funding')\n",
    "\n",
    "# Fetch the last sequence for each municipality in the future data\n",
    "predictions = {}\n",
    "for code in scaled_future_data['Codigo_IBGE'].unique():\n",
    "    last_sequence = get_last_sequence_for_municipality(code, sequences_padded, scaled_df)\n",
    "    \n",
    "    # Extend the last sequence with the new data point\n",
    "    future_point = scaled_future_data[scaled_future_data['Codigo_IBGE'] == code].drop(['Ano', 'Codigo_IBGE'], axis=1).values\n",
    "\n",
    "    extended_sequence = np.vstack([last_sequence, future_point])\n",
    "    \n",
    "    # Pad the sequence\n",
    "    extended_sequence_padded = pad_sequences([extended_sequence])\n",
    "    \n",
    "    # Make the prediction\n",
    "    prediction = mymodel.predict(extended_sequence_padded)\n",
    "    prediction_inverse_transformed = inverse_transform_prediction(prediction)\n",
    "    predictions[code] = prediction_inverse_transformed\n",
    "    \n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297f1799-7126-4e03-ab66-edb1eceaecbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236d74a7-5943-4f67-b649-207e6c235ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3747fab0-0d77-43a8-809b-2337afb9ad9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2452fa-c895-4566-bae6-313a67310761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c4a3e1-0634-44f7-b690-dc54be2f0973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
